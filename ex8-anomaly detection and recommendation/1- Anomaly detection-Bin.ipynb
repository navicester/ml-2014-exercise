{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# note:\n",
    "* [covariance matrix](http://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html)\n",
    "* [multivariate_normal](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.multivariate_normal.html)\n",
    "* [seaborn  bivariate kernel density estimate](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.kdeplot.html#seaborn.kdeplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\", style=\"white\", palette=sns.color_palette(\"RdBu\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from scipy import stats\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from helper import anomaly\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to divide data into 3 set. \n",
    "1. Training set\n",
    "2. Cross Validation set\n",
    "3. Test set.  \n",
    "\n",
    "You shouldn't be doing prediction using training data or Validation data as it does in the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = sio.loadmat('./data/ex8data1.mat')\n",
    "mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mat.get('X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "divide original validation data into validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval, Xtest, yval, ytest = train_test_split(mat.get('Xval'),\n",
    "                                            mat.get('yval').ravel(),\n",
    "                                            test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot('Latency', 'Throughput',\n",
    "           data=pd.DataFrame(X, columns=['Latency', 'Throughput']), \n",
    "           fit_reg=False,\n",
    "           scatter_kws={\"s\":20,\n",
    "                        \"alpha\":0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# estimate multivariate Gaussian parameters $\\mu$ and $\\sigma^2$\n",
    "> according to data, X1, and X2 is not independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = X.mean(axis=0)\n",
    "print(mu, '\\n')\n",
    "\n",
    "cov = np.cov(X.T)\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of creating 2d grid to calculate probability density\n",
    "np.dstack(np.mgrid[0:3,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multi-var Gaussian model\n",
    "multi_normal = stats.multivariate_normal(mu, cov)\n",
    "\n",
    "# create a grid\n",
    "x, y = np.mgrid[0:30:0.01, 0:30:0.01]\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot probability density\n",
    "ax.contourf(x, y, multi_normal.pdf(pos), cmap='Blues')\n",
    "\n",
    "# plot original data points\n",
    "sns.regplot('Latency', 'Throughput',\n",
    "           data=pd.DataFrame(X, columns=['Latency', 'Throughput']), \n",
    "           fit_reg=False,\n",
    "           ax=ax,\n",
    "           scatter_kws={\"s\":10,\n",
    "                        \"alpha\":0.4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select threshold $\\epsilon$\n",
    "1. use training set $X$ to model the multivariate Gaussian\n",
    "2. use cross validation set $(Xval, yval)$ to find the best $\\epsilon$ by finding the best `F-score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"../img/f1_score.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_threshold(X, Xval, yval):\n",
    "    \"\"\"use CV data to find the best epsilon\n",
    "    Returns:\n",
    "        e: best epsilon with the highest f-score\n",
    "        f-score: such best f-score\n",
    "    \"\"\"\n",
    "    # create multivariate model using training data\n",
    "    mu = X.mean(axis=0)\n",
    "    cov = np.cov(X.T)\n",
    "    multi_normal = stats.multivariate_normal(mu, cov)\n",
    "\n",
    "    # this is key, use CV data for fine tuning hyper parameters\n",
    "    pval = multi_normal.pdf(Xval)\n",
    "\n",
    "    # set up epsilon candidates\n",
    "    epsilon = np.linspace(np.min(pval), np.max(pval), num=10000)\n",
    "\n",
    "    # calculate f-score\n",
    "    fs = []\n",
    "    for e in epsilon:\n",
    "        y_pred = (pval <= e).astype('int')\n",
    "        fs.append(f1_score(yval, y_pred))\n",
    "\n",
    "    # find the best f-score\n",
    "    argmax_fs = np.argmax(fs)\n",
    "\n",
    "    return epsilon[argmax_fs], fs[argmax_fs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, fs = select_threshold(X, Xval, yval)\n",
    "print('Best epsilon: {}\\nBest F-score on validation data: {}'.format(e, fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize prediction of `Xval` using learned $\\epsilon$\n",
    "1. use CV data to find the best $\\epsilon$\n",
    "2. use all data (training + validation) to create model\n",
    "3. do the prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_normal, y_pred = anomaly.predict(X, Xval, e, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct test DataFrame\n",
    "data = pd.DataFrame(Xtest, columns=['Latency', 'Throughput'])\n",
    "data['y_pred'] = y_pred\n",
    "\n",
    "# create a grid for graphing\n",
    "x, y = np.mgrid[0:30:0.01, 0:30:0.01]\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot probability density\n",
    "ax.contourf(x, y, multi_normal.pdf(pos), cmap='Blues')\n",
    "\n",
    "# plot original Xval points\n",
    "sns.regplot('Latency', 'Throughput',\n",
    "            data=data,\n",
    "            fit_reg=False,\n",
    "            ax=ax,\n",
    "            scatter_kws={\"s\":10,\n",
    "                         \"alpha\":0.4})\n",
    "\n",
    "# mark the predicted anamoly of CV data. We should have a test set for this...\n",
    "anamoly_data = data[data['y_pred']==1]\n",
    "ax.scatter(anamoly_data['Latency'], anamoly_data['Throughput'], marker='x', s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# high dimension data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = sio.loadmat('./data/ex8data2.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mat.get('X')\n",
    "Xval, Xtest, yval, ytest = train_test_split(mat.get('Xval'),\n",
    "                                            mat.get('yval').ravel(),\n",
    "                                            test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, fs = anomaly.select_threshold(X, Xval, yval)\n",
    "print('Best epsilon: {}\\nBest F-score on validation data: {}'.format(e, fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_normal, y_pred = anomaly.predict(X, Xval, e, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('find {} anamolies'.format(y_pred.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huge difference between my result, and the official `117` anamolies in the ex8 is due to:\n",
    "1. my use of **multivariate Gaussian**\n",
    "2. I split data very differently"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
